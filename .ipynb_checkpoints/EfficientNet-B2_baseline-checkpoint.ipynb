{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet-B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.3.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\miniconda3\\envs\\carnd\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random', 'datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import math\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow_model.efficientnet import EfficientNet\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.3.0 or above.\"\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to other families by referencing: \n",
    "# ~\\miniconda3\\envs\\carnd\\Lib\\site-packages\\tensorflow\\python\\keras\\applications\\efficientnet.py\n",
    "\n",
    "def EfficientNetB2(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   classifier_activation='softmax',\n",
    "                   **kwargs):\n",
    "    return EfficientNet(\n",
    "        1.1,\n",
    "        1.2,\n",
    "        260,\n",
    "        0.3,\n",
    "        model_name='efficientnetb2',\n",
    "        include_top=include_top,\n",
    "        weights=weights,\n",
    "        input_tensor=input_tensor,\n",
    "        input_shape=input_shape,\n",
    "        pooling=pooling,\n",
    "        classes=classes,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_imgs (InputLayer)      [(None, 6, 128, 256)]     0         \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 128, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "efficientnetb2 (Functional)  (None, 4, 8, 1408)        7769439   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 4, 8, 32)          405536    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "elu (ELU)                    (None, 4, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "vision_features (Flatten)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense1_relu (ReLU)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense2_relu (ReLU)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 8,856,931\n",
      "Trainable params: 8,789,286\n",
      "Non-trainable params: 67,645\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_input_data = keras.layers.Input(shape=(6, 128, 256), name=\"input_imgs\")\n",
    "permute1 = keras.layers.Permute((2,3,1))(img_input_data)\n",
    "efficientnet = EfficientNetB2(include_top=False, weights = None, input_shape=(128,256,6))(permute1)\n",
    "conv2d1 = tf.keras.layers.Conv2D(32, 3, padding='same')(efficientnet)\n",
    "normalization = keras.layers.BatchNormalization()(conv2d1)\n",
    "elu1 = tf.keras.layers.ELU(alpha=1.0)(normalization)\n",
    "flatten1 = tf.keras.layers.Flatten(name=\"vision_features\")(elu1)\n",
    "\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(512)(flatten1)\n",
    "dense1_relu = tf.keras.layers.ReLU(name=\"dense1_relu\")(dense1)\n",
    "#droupout1 = tf.keras.layers.Dropout(0.2)(dense1_relu)\n",
    "dense2 = tf.keras.layers.Dense(256)(dense1_relu)\n",
    "dense2_relu = tf.keras.layers.ReLU(name=\"dense2_relu\")(dense2)\n",
    "#droupout2 = tf.keras.layers.Dropout(0.2)(dense2_relu)\n",
    "path = tf.keras.layers.Dense(100)(dense2_relu)\n",
    "\n",
    "output = path\n",
    "model = keras.models.Model(inputs=[img_input_data], outputs=[output])\n",
    "\n",
    "# Renaming all layers which deals with image\n",
    "\"\"\"\n",
    "for layer in model.layers:\n",
    "    layer._name = \"vision_\" + layer.name\n",
    "\"\"\"\n",
    "    \n",
    "#concat = keras.layers.concatenate([input_data, hidden2])\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"training_data/baseline_batch/\"\n",
    "x_training = np.load(DATA_PATH + \"input.npy\")\n",
    "y_training = np.load(DATA_PATH + \"output.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 2/63 [..............................] - ETA: 26s - loss: 2.7280 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0795s vs `on_train_batch_end` time: 0.7565s). Check your callbacks.\n",
      "63/63 [==============================] - 20s 317ms/step - loss: 2.1959 - accuracy: 0.1916 - val_loss: 0.2651 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 15s 231ms/step - loss: 1.8261 - accuracy: 0.1756 - val_loss: 0.2341 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 15s 231ms/step - loss: 2.1111 - accuracy: 0.1557 - val_loss: 585728.3750 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 2.1659 - accuracy: 0.0739 - val_loss: 156.1109 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 66s 1s/step - loss: 1.6814 - accuracy: 0.1457 - val_loss: 104.7599 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Model checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('efficientnet_v1.h5', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=200, monitor='val_loss'), tf.keras.callbacks.TensorBoard(log_dir='logs')]\n",
    "\n",
    "results = model.fit(x_training, y_training, validation_split=0.10, batch_size=8, epochs=500, callbacks=callbacks)\n",
    "model.save(\"efficientnet_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable import from different directory\n",
    "import sys\n",
    "sys.path.insert(0, 'D:/comma2k19_dataset/notebooks/lib/')\n",
    "from common.transformations.camera import transform_img, eon_intrinsics\n",
    "from common.transformations.model import medmodel_intrinsics\n",
    "\n",
    "\n",
    "VIDEO_PATH = 'D:/comma2k19_dataset/10/video.hevc'\n",
    "\n",
    "TOTAL_FRAMES = 1200\n",
    "bgr_imgs = []\n",
    "yuv_imgs = []\n",
    "\n",
    "\n",
    "vid = cv2.VideoCapture(VIDEO_PATH)\n",
    "for frame_number in tqdm(range(TOTAL_FRAMES)):\n",
    "    ret, frame = vid.read()\n",
    "    if (frame_number % 5 == 0):\n",
    "        bgr_imgs.append(frame)\n",
    "        frame = cv2.resize(frame,(1164,874))\n",
    "        img_yuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV_I420)\n",
    "        yuv_imgs.append(img_yuv)\n",
    "\n",
    "\n",
    "\n",
    "def frames_to_tensor(frames):                                                                                               \n",
    "    H = (frames.shape[1]*2)//3                                                                                                \n",
    "    W = frames.shape[2]                                                                                                       \n",
    "    in_img1 = np.zeros((frames.shape[0], 6, H//2, W//2), dtype=np.uint8)                                                      \n",
    "                                                                                                                            \n",
    "    in_img1[:, 0] = frames[:, 0:H:2, 0::2]                                                                                    \n",
    "    in_img1[:, 1] = frames[:, 1:H:2, 0::2]                                                                                    \n",
    "    in_img1[:, 2] = frames[:, 0:H:2, 1::2]                                                                                    \n",
    "    in_img1[:, 3] = frames[:, 1:H:2, 1::2]                                                                                    \n",
    "    in_img1[:, 4] = frames[:, H:H+H//4].reshape((-1, H//2,W//2))                                                              \n",
    "    in_img1[:, 5] = frames[:, H+H//4:H+H//2].reshape((-1, H//2,W//2))\n",
    "    return in_img1\n",
    "\n",
    "imgs_med_model = np.zeros((len(yuv_imgs), 384, 512), dtype=np.uint8)\n",
    "for i, img in tqdm(enumerate(yuv_imgs)):\n",
    "    imgs_med_model[i] = transform_img(img, from_intr=eon_intrinsics, to_intr=medmodel_intrinsics, yuv=True,\n",
    "                                    output_size=(512,256))\n",
    "frame_tensors = frames_to_tensor(np.array(imgs_med_model)).astype(np.float32)/128.0 - 1.0\n",
    "\n",
    "figsize(12,12);\n",
    "imshow(bgr_imgs[0]);\n",
    "title('First Image', fontsize=25);\n",
    "\n",
    "outs = model.predict(frame_tensors, verbose=1)\n",
    "data_length = len(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verification_folder = \"validation\"\n",
    "# Get the length of the data\n",
    "\n",
    "for i in tqdm(range(data_length)):\n",
    "    verification_img = bgr_imgs[i]\n",
    "    forward_data = np.linspace(0,99,100)\n",
    "    down_data = np.zeros(100)\n",
    "    draw_path(np.transpose([forward_data,outs[i],down_data]), img=verification_img, isTest = True)\n",
    "        \n",
    "    cv2.imwrite(os.path.join(verification_folder , str(i) + '.jpg'), verification_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
