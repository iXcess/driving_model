{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ConvNet Baseline\n",
    "This simple model is used as baseline to be compared with other models to be constructed. It acts as a simple benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "# Used to split test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Keras is a high level wrapper on top of tensorflow (machine learning library)\n",
    "# The Sequential container is a linear stack of layers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "# Popular optimization strategy that uses gradient descent \n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "# To save our model periodically as checkpoints for loading later\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Types of layers do we want our model to have\n",
    "from tensorflow.python.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Cropping2D, BatchNormalization, ELU\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value= 523\n",
    "    \n",
    "def reset_random_seed():\n",
    "    '''\n",
    "    Set all the random seed generator to a fixed value to reproduce the same results at every training\n",
    "    '''\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.compat.v1.set_random_seed(seed_value)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"training_data/baseline_batch/\"\n",
    "data = \"path\"\n",
    "x_training = np.load(DATA_PATH + \"input.npy\")\n",
    "y_training = np.load(DATA_PATH + \"output.npy\")\n",
    "\n",
    "INPUT_SHAPE = np.shape(x_training)[1:]\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(data = \"path\"):\n",
    "    \"\"\"\n",
    "    NVIDIA model used, referenced as a starting point\n",
    "    Image normalization to avoid saturation and make gradients work better.\n",
    "    Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
    "    Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
    "    Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
    "    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "    Drop out (0.5)\n",
    "    Fully connected: neurons: 100, activation: ELU\n",
    "    Fully connected: neurons: 50, activation: ELU\n",
    "    Fully connected: neurons: 10, activation: ELU\n",
    "    Fully connected: neurons: 1 (output)\n",
    "\n",
    "    ELU(Exponential linear unit) function takes care of the Vanishing gradient problem. \n",
    "    \"\"\"\n",
    "    #reset_random_seed()\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Image normalization to avoid saturation and make gradients work better.\n",
    "    #model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Lambda(lambda x: x /255.0 - 0.5, input_shape=INPUT_SHAPE ))\n",
    "    # Convolutions\n",
    "    model.add(Conv2D(24, 5, strides=(2, 2), padding = \"same\", activation='elu'))\n",
    "    model.add(Conv2D(36, 3, strides=(2, 2), padding = \"same\", activation='elu'))\n",
    "    model.add(Conv2D(256, 3, strides=(1, 1), padding = \"same\", activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    # Drop out (0.5)\n",
    "    model.add(Dropout(0.5, seed=seed_value))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if data == \"path\":\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.1, seed=seed_value))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    else:\n",
    "        # FCNs\n",
    "        model.add(Dense(100, activation='elu'))\n",
    "        model.add(Dense(50, activation='elu'))\n",
    "        model.add(Dense(10, activation='elu'))\n",
    "        model.add(Dense(1, activation='elu'))\n",
    "        \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 6, 128, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 3, 64, 24)         153624    \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 32, 36)         7812      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 32, 256)        83200     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2, 32, 256)        1024      \n",
      "_________________________________________________________________\n",
      "elu (ELU)                    (None, 2, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 8,686,080\n",
      "Trainable params: 8,685,568\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = construct_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dumb baseline, we should make sure the following:\n",
    "    1. Random seed is fixed so that the data is training to reproduce the same thing very single time. This removes a factor of variation and will help keep you sane. For more information, read [here](https://medium.com/@ODSC/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752) CHECKED\n",
    "    2. Simplify the data, no augmentation is done. CHECKED\n",
    "    3. Verify init loss. Make sure the loss starts at the correct loss value. CHECKED\n",
    "    4. Init well so that the first few epochs don't need to waste the time to learn the biases. We know from the data processing that the path data has a mean around 1, hence the default initialiser is good enough. CHECKED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 5.3313\n",
      "Epoch 00001: val_loss improved from inf to 3.04270, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 5.3313 - val_loss: 3.0427\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.6146\n",
      "Epoch 00002: val_loss did not improve from 3.04270\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 3.6146 - val_loss: 3.1422\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.1799\n",
      "Epoch 00003: val_loss improved from 3.04270 to 1.33940, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 3.1799 - val_loss: 1.3394\n",
      "Epoch 4/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.8359\n",
      "Epoch 00004: val_loss improved from 1.33940 to 0.94715, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 2.8303 - val_loss: 0.9471\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.7726\n",
      "Epoch 00005: val_loss improved from 0.94715 to 0.73490, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 2.7726 - val_loss: 0.7349\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.8026\n",
      "Epoch 00006: val_loss improved from 0.73490 to 0.50588, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 2.8026 - val_loss: 0.5059\n",
      "Epoch 7/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.7190\n",
      "Epoch 00007: val_loss improved from 0.50588 to 0.41339, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 2.6877 - val_loss: 0.4134\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.5981\n",
      "Epoch 00008: val_loss improved from 0.41339 to 0.30039, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 2.5981 - val_loss: 0.3004\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.7926\n",
      "Epoch 00009: val_loss did not improve from 0.30039\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 2.7926 - val_loss: 1.0476\n",
      "Epoch 10/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.8167\n",
      "Epoch 00010: val_loss did not improve from 0.30039\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 2.7661 - val_loss: 0.4438\n",
      "Epoch 11/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.5431\n",
      "Epoch 00011: val_loss improved from 0.30039 to 0.28071, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 2.5494 - val_loss: 0.2807\n",
      "Epoch 12/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.5418\n",
      "Epoch 00012: val_loss did not improve from 0.28071\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 2.5595 - val_loss: 0.3320\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.5981\n",
      "Epoch 00013: val_loss did not improve from 0.28071\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 2.5981 - val_loss: 0.4639\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.6688\n",
      "Epoch 00014: val_loss did not improve from 0.28071\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 2.6688 - val_loss: 0.3534\n",
      "Epoch 15/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.6253\n",
      "Epoch 00015: val_loss did not improve from 0.28071\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 2.6344 - val_loss: 0.2934\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4969\n",
      "Epoch 00016: val_loss improved from 0.28071 to 0.24035, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 2.4969 - val_loss: 0.2403\n",
      "Epoch 17/20\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.5453\n",
      "Epoch 00017: val_loss did not improve from 0.24035\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 2.5525 - val_loss: 0.4153\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4529\n",
      "Epoch 00018: val_loss improved from 0.24035 to 0.17888, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 2.4529 - val_loss: 0.1789\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4227\n",
      "Epoch 00019: val_loss did not improve from 0.17888\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 2.4227 - val_loss: 0.1873\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4734\n",
      "Epoch 00020: val_loss improved from 0.17888 to 0.17707, saving model to simple_v1.h5\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 2.4734 - val_loss: 0.1771\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# Using adam optimizer and also mean squared error as the loss function\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer='adam', loss=\"mse\")\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"simple_v1.h5\", monitor='val_loss', verbose=1,\n",
    "                                  save_best_only=True, mode='min')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.0001, patience=50,\n",
    "                                verbose=1, mode='min')\n",
    "\n",
    "\n",
    "model.fit(x_training, y_training, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                      callbacks=[checkpoint, early_stop], validation_split=0.18, shuffle=True)\n",
    "\n",
    "model.save('simple_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
